{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Can be used for both binary classification and multi-class classification\n",
    "- We also get the probability of belonging in a class\n",
    "\n",
    "### When to use?\n",
    "- If the data is binary (0/1, Yes/No etc.)\n",
    "- If you need probabilistic results\n",
    "- If your data is \"linearly separable\" - meaning if using linear boundries between classes would work\n",
    "- If you need to understand the impact of a feature. (Age's impact in voting Democrat etc.)\n",
    "\n",
    "### Linear Regression vs Logistic Regression\n",
    "- Linear regression does not give us a probability, it just goes to 0 if y < 0.5 and goes to 1 if y >= 0.5\n",
    "- Instead of (Theta_transposeX) = (Theta0 + Theta1x1 + ... + Theta_n * x_n);\n",
    "    - We should use Sigmoid(Theta_transposeX) = Sigmoid(Theta0 + Theta1x1 + ... + Theta_n * x_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Function\n",
    "\n",
    "- Sigmoid(Theta_transposeX) = 1 / (1 + e^(-Theta_transposeX))\n",
    "- As Theta_transposeX increases, Y approaches to 1.\n",
    "- As Theta_transposeX decreases, Y approaches to 0.\n",
    "- If you're trying to estimate churn by income and age:\n",
    "    - P(Y=1|X) -- Probability of Y = 1, given X\n",
    "    - P(Churn=1|income, age) -- Probability of Churn = 1, given age and income\n",
    "\n",
    "### The Training Process\n",
    "- Initialize Theta vector with random values, as with most ML algorithms. -> Theta = [-1, 2]\n",
    "- Calculate Y = Sigmoid(Theta_transposeX) for a customer\n",
    "    - Y = Sigmoid([-1, 2] * [2, 5]) = 0.7\n",
    "- Compare the output with the actual label. If label is 1, then the error is 0.3\n",
    "- Calculate the error for all customers, then add them up. **This is the cost.**\n",
    "- Change Theta values to reduce the cost, start over until the cost is low enough.\n",
    "- **We can use gradient descent to estimate the most accurate Theta values.**\n",
    "- **Stopping the iteration is up to us, we can stop it when we get a satisfactory cost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Cost Function\n",
    "\n",
    "- Y = Sigmoid(Theta_transposeX)\n",
    "- Cost(Y, y) = (Y - y)^2 / 2\n",
    "- **J(Theta) = Avg(Cost(Y, y))**\n",
    "- We should calculate the minimum point of this function to show the best parameters by Gradient Descent\n",
    "- An easier way:\n",
    "    - If desirable y = 1, cost: **-log(Y)**\n",
    "    - If desirable y = 0, cost: **-log(1 - Y)**\n",
    "    - So the cost function is:\n",
    "    - ![img](coostt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gradient Descent to minimize the Cost\n",
    "\n",
    "- Gradient: Slope at a specific point\n",
    "- If the gradient is decreasing at every step, we're in the right direction\n",
    "- ![gradient descent](jkjk.png)\n",
    "- Partial derivative gives the slope at a point\n",
    "- If we calculate the derivative of the J at Theta1, if it's a positive number we need to change direction.\n",
    "- If the slope is large, we should take a larger step. Therefore at each iteration, the \"size\" of the step eventually decreases.\n",
    "- ![j derivative](jder.png)\n",
    "- There is a vector VJ that is formed by the derivative of J at Theta1\n",
    "    - **NewTheta = OldTheta - mu * VJ**\n",
    "    - **mu** is the learning rate, the length of the step we take"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
